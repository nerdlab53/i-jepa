{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "274fd365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/himanshubhenwal/miniforge3/envs/torch2/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/himanshubhenwal/miniforge3/envs/torch2/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /Users/himanshubhenwal/miniforge3/envs/torch2/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <552B36CA-07A6-332B-BF7F-6D22D9005F71> /Users/himanshubhenwal/miniforge3/envs/torch2/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from utils import print_tensor_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0723576",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2, 3, 4)\n",
    "pred_features = torch.rand(shape)\n",
    "target_features = torch.rand(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "781de692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8649, 0.5216, 0.6526, 0.9176],\n",
       "         [0.8061, 0.0014, 0.2069, 0.8448],\n",
       "         [0.9477, 0.2697, 0.5624, 0.7117]],\n",
       "\n",
       "        [[0.6907, 0.2050, 0.5935, 0.4493],\n",
       "         [0.8745, 0.7627, 0.5185, 0.7344],\n",
       "         [0.1786, 0.0843, 0.0201, 0.8117]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e391e03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5644, 0.0294, 0.6311, 0.6940],\n",
       "         [0.4751, 0.0606, 0.8409, 0.2881],\n",
       "         [0.8621, 0.3122, 0.8568, 0.2087]],\n",
       "\n",
       "        [[0.5362, 0.9477, 0.3087, 0.8687],\n",
       "         [0.9727, 0.1991, 0.8457, 0.8976],\n",
       "         [0.1155, 0.5781, 0.9644, 0.7395]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866bd777",
   "metadata": {},
   "source": [
    "## Creating a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abd0b788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "batch_size, max_targets, embed_dim = shape\n",
    "mask = torch.zeros(batch_size, max_targets, dtype=torch.bool)\n",
    "print(mask)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2d8a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_targets_per_batch = [2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1c498a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(batch_size):\n",
    "    mask[b, :num_targets_per_batch[b]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aed8fb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True, False],\n",
       "        [ True,  True,  True]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d44bac8",
   "metadata": {},
   "source": [
    "## Normalizing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49f6c546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[7.4799e-01, 2.7204e-01, 4.2586e-01, 8.4193e-01],\n",
      "         [6.4973e-01, 1.8391e-06, 4.2828e-02, 7.1363e-01],\n",
      "         [8.9809e-01, 7.2744e-02, 3.1631e-01, 5.0648e-01]],\n",
      "\n",
      "        [[4.7701e-01, 4.2029e-02, 3.5221e-01, 2.0183e-01],\n",
      "         [7.6468e-01, 5.8169e-01, 2.6884e-01, 5.3928e-01],\n",
      "         [3.1893e-02, 7.1122e-03, 4.0231e-04, 6.5891e-01]]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "pred_features_squared = pred_features**2\n",
    "print(pred_features_squared)\n",
    "print(pred_features_squared.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74da660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2.2878],\n",
      "         [1.4062],\n",
      "         [1.7936]],\n",
      "\n",
      "        [[1.0731],\n",
      "         [2.1545],\n",
      "         [0.6983]]])\n",
      "torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "sum_squared = torch.sum(pred_features_squared, dim=-1, keepdims=True)\n",
    "print(sum_squared)\n",
    "print(sum_squared.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a567c9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.5126],\n",
      "         [1.1858],\n",
      "         [1.3393]],\n",
      "\n",
      "        [[1.0359],\n",
      "         [1.4678],\n",
      "         [0.8357]]])\n",
      "torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "pred_norm = torch.sqrt(sum_squared)\n",
    "print(pred_norm)\n",
    "print(pred_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc8f242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5718, 0.3448, 0.4314, 0.6066],\n",
      "         [0.6797, 0.0011, 0.1745, 0.7124],\n",
      "         [0.7076, 0.2014, 0.4199, 0.5314]],\n",
      "\n",
      "        [[0.6667, 0.1979, 0.5729, 0.4337],\n",
      "         [0.5958, 0.5196, 0.3532, 0.5003],\n",
      "         [0.2137, 0.1009, 0.0240, 0.9714]]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "normalized_pred_features = pred_features / pred_norm\n",
    "print(normalized_pred_features)\n",
    "print(normalized_pred_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfe773b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5154, 0.0268, 0.5763, 0.6337],\n",
      "         [0.4705, 0.0600, 0.8328, 0.2853],\n",
      "         [0.6777, 0.2454, 0.6735, 0.1641]],\n",
      "\n",
      "        [[0.3758, 0.6643, 0.2164, 0.6089],\n",
      "         [0.6144, 0.1257, 0.5342, 0.5669],\n",
      "         [0.0855, 0.4280, 0.7140, 0.5475]]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "target_features_squared = target_features**2\n",
    "sum_squared = torch.sum(target_features_squared, dim=-1, keepdims=True)\n",
    "target_norm = torch.sqrt(sum_squared)\n",
    "normalized_target_features = target_features / target_norm\n",
    "print(normalized_target_features)\n",
    "print(normalized_target_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593c4c9",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6358956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3.1827e-03, 1.0112e-01, 2.0974e-02, 7.3297e-04],\n",
      "         [4.3774e-02, 3.4608e-03, 4.3337e-01, 1.8237e-01],\n",
      "         [8.9553e-04, 1.9393e-03, 6.4286e-02, 1.3492e-01]],\n",
      "\n",
      "        [[8.4644e-02, 2.1748e-01, 1.2712e-01, 3.0691e-02],\n",
      "         [3.4589e-04, 1.5514e-01, 3.2733e-02, 4.4409e-03],\n",
      "         [1.6440e-02, 1.0699e-01, 4.7610e-01, 1.7971e-01]]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "loss_per_element = loss_fn(normalized_pred_features, normalized_target_features)\n",
    "print(loss_per_element)\n",
    "print(loss_per_element.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a84bf9",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT) Sequential Flow\n",
    "\n",
    "### 1. Image to Patches (PatchEmbed)\n",
    "- **Input:** Image tensor (B, C, H, W)\n",
    "- **Process:** \n",
    "  - Apply Conv2d with kernel_size=patch_size, stride=patch_size\n",
    "  - (B, C, H, W) → (B, embed_dim, H//patch_size, W//patch_size)\n",
    "  - Flatten spatial dimensions: (B, embed_dim, H//patch_size, W//patch_size) → (B, embed_dim, N) where N=(H×W)/(patch_size²)\n",
    "  - Transpose: (B, embed_dim, N) → (B, N, embed_dim)\n",
    "- **Output:** Patch tokens (B, N, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "461010e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \n",
    "    def __init__(self, patch_size : int = 16, img_size : int = 224, in_chans : int = 3, embed_dim : int = 768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_patches = (img_size // patch_size)\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        assert H == W == self.img_size, f\"Input image size doesn't match with model size.\"\n",
    "\n",
    "        # (B, C, H, W) -> (B, embed_dim, H//patch_size, W//patch_size)\n",
    "        # (B, embed_dim, H', W') -> (B, embed_dim, N) N = (HxW)/(patch_size)^2 = (H'xW')\n",
    "        # (B, embed_dim, N) -> (B, N, embed_dim)\n",
    "        x = rearrange(self.proj(x), 'b c h w -> b (h w) c')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fc80efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------+\n",
      "| Property      | Value                 |\n",
      "+===============+=======================+\n",
      "| Name          | tensor                |\n",
      "+---------------+-----------------------+\n",
      "| Shape         | (1, 3, 224, 224)      |\n",
      "+---------------+-----------------------+\n",
      "| Dimensions    | 4                     |\n",
      "+---------------+-----------------------+\n",
      "| Dtype         | torch.float32         |\n",
      "+---------------+-----------------------+\n",
      "| Device        | cpu                   |\n",
      "+---------------+-----------------------+\n",
      "| Min           | 1.138448715209961e-05 |\n",
      "+---------------+-----------------------+\n",
      "| Max           | 0.9999926090240479    |\n",
      "+---------------+-----------------------+\n",
      "| Mean          | 0.4993129372596741    |\n",
      "+---------------+-----------------------+\n",
      "| Std           | 0.2894613444805145    |\n",
      "+---------------+-----------------------+\n",
      "| Memory (MB)   | 0.57421875            |\n",
      "+---------------+-----------------------+\n",
      "| Requires Grad | False                 |\n",
      "+---------------+-----------------------+\n",
      "| Has NaN       | False                 |\n",
      "+---------------+-----------------------+\n",
      "| Has Inf       | False                 |\n",
      "+---------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "p = PatchEmbed()\n",
    "t = torch.rand(1, 3, 224, 224)\n",
    "print_tensor_info(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d859e47b",
   "metadata": {},
   "source": [
    "## Multi-head Self-Attention\n",
    "- **Input:** Normalized tokens (B, N+1, embed_dim)\n",
    "- **Process:**\n",
    "  - QKV projection: (B, N+1, embed_dim) → (B, N+1, 3×embed_dim) → reshape to (B, N+1, 3, num_heads, head_dim)\n",
    "  - Permute to (3, B, num_heads, N+1, head_dim) and split into Q, K, V\n",
    "  - Calculate attention: (q @ k.transpose) * scale → (B, num_heads, N+1, N+1)\n",
    "  - Apply softmax and dropout\n",
    "  - Apply attention to values: (B, num_heads, N+1, head_dim)\n",
    "  - Reshape and project: (B, N+1, embed_dim)\n",
    "- **Output:** Self-attention output (B, N+1, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82de5d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads : int = 8, qkv_bias : bool = True, attn_drop : float = 0., proj_drop : float = 0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim = dim\n",
    "        self.scale = dim**-0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias = qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        qkv = rearrange(self.qkv(x), 'b n (t h d) -> t b h n d', t = 3, h = self.num_heads)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1))*self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = rearrange(attn @ v, 'b h n d -> b n (h d)')\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a06fc895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : \n",
      "tensor([[[ 0.6379, -1.7139,  0.5090, -1.4304, -0.1238,  0.0871],\n",
      "         [-1.5324, -0.4285,  0.8974, -0.0831,  1.1962,  0.7543],\n",
      "         [-1.1553,  0.4374, -1.0537,  1.9426,  1.2371,  0.8994],\n",
      "         [ 0.4621,  1.0332,  0.0703, -2.1739, -1.3538,  0.3809]],\n",
      "\n",
      "        [[ 0.5076,  1.6884,  0.4170, -0.5509, -1.3361, -2.7045],\n",
      "         [-1.4617, -0.5990,  0.9987, -0.8241, -0.3655,  0.8913],\n",
      "         [-0.6579, -1.5810,  1.3470,  1.1436,  1.1443,  0.7243],\n",
      "         [-0.4441,  0.2662,  1.1105, -0.3284,  0.7214, -1.1463]]])\n",
      "Input shape: torch.Size([2, 4, 6])\n",
      "QKV Projected : \n",
      "tensor([[[ 0.7425, -0.0707, -0.0828,  0.4573,  0.4544,  1.0330,  0.2471,\n",
      "           0.0914,  0.7612, -0.0415,  1.4642,  0.0638, -1.0289,  0.5725,\n",
      "           0.2907,  0.3723,  0.7210,  0.5929],\n",
      "         [ 0.3572, -0.8977,  0.0735, -0.6721,  0.5857,  0.6755,  1.1203,\n",
      "          -0.0960, -0.2443,  0.9797, -0.5174, -0.4123, -0.8497, -0.5494,\n",
      "           1.0104,  0.8182, -0.1323,  0.0854],\n",
      "         [-0.4196, -0.7100, -0.6434, -1.4602,  0.5826,  0.2719,  0.6467,\n",
      "          -0.7578, -0.8227,  1.5211, -1.2511, -1.5030, -0.3799, -1.5127,\n",
      "          -0.4446, -0.3233, -0.8016, -0.7996],\n",
      "         [ 1.4142, -0.4784,  1.2242,  1.7068,  1.0256,  0.1558, -0.7472,\n",
      "           0.5901, -0.2340, -0.6541,  0.6875,  0.2207,  0.1473,  0.6119,\n",
      "           0.4588, -0.8222,  0.7653,  0.7958]],\n",
      "\n",
      "        [[ 0.5831,  0.2470,  1.2699,  0.5537, -0.2212, -0.1294, -0.7502,\n",
      "           0.9520, -1.5822, -1.1268,  1.1407,  0.5921,  0.9926,  0.8681,\n",
      "          -0.3637, -0.0956,  0.2703,  1.4685],\n",
      "         [ 0.9850, -0.6415,  0.7407,  0.1534,  0.8821,  0.7912,  0.7052,\n",
      "           0.2720, -0.3689,  0.4200,  0.1327, -0.1222, -0.5099, -0.1323,\n",
      "           1.0253,  0.4142,  0.4203,  0.7151],\n",
      "         [ 0.0825, -0.5945, -0.6763, -1.0154, -0.1117,  0.6730,  1.5108,\n",
      "          -0.7995, -0.1226,  1.3568,  0.2071, -0.4168, -0.6232, -0.8120,\n",
      "           0.3218,  0.9828, -0.4855,  0.0317],\n",
      "         [ 0.2901, -0.4221,  0.3713, -0.3297, -0.0722,  0.3486,  0.5429,\n",
      "           0.3209, -0.6252,  0.0951,  0.3322,  0.2183, -0.2394,  0.1995,\n",
      "           0.5550,  0.8840, -0.0611,  0.6710]]], grad_fn=<ViewBackward0>)\n",
      "After QKV projection: torch.Size([2, 4, 18])\n",
      "QKV : \n",
      "tensor([[[[[ 0.7425, -0.0707, -0.0828],\n",
      "           [ 0.3572, -0.8977,  0.0735],\n",
      "           [-0.4196, -0.7100, -0.6434],\n",
      "           [ 1.4142, -0.4784,  1.2242]],\n",
      "\n",
      "          [[ 0.4573,  0.4544,  1.0330],\n",
      "           [-0.6721,  0.5857,  0.6755],\n",
      "           [-1.4602,  0.5826,  0.2719],\n",
      "           [ 1.7068,  1.0256,  0.1558]]],\n",
      "\n",
      "\n",
      "         [[[ 0.5831,  0.2470,  1.2699],\n",
      "           [ 0.9850, -0.6415,  0.7407],\n",
      "           [ 0.0825, -0.5945, -0.6763],\n",
      "           [ 0.2901, -0.4221,  0.3713]],\n",
      "\n",
      "          [[ 0.5537, -0.2212, -0.1294],\n",
      "           [ 0.1534,  0.8821,  0.7912],\n",
      "           [-1.0154, -0.1117,  0.6730],\n",
      "           [-0.3297, -0.0722,  0.3486]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.2471,  0.0914,  0.7612],\n",
      "           [ 1.1203, -0.0960, -0.2443],\n",
      "           [ 0.6467, -0.7578, -0.8227],\n",
      "           [-0.7472,  0.5901, -0.2340]],\n",
      "\n",
      "          [[-0.0415,  1.4642,  0.0638],\n",
      "           [ 0.9797, -0.5174, -0.4123],\n",
      "           [ 1.5211, -1.2511, -1.5030],\n",
      "           [-0.6541,  0.6875,  0.2207]]],\n",
      "\n",
      "\n",
      "         [[[-0.7502,  0.9520, -1.5822],\n",
      "           [ 0.7052,  0.2720, -0.3689],\n",
      "           [ 1.5108, -0.7995, -0.1226],\n",
      "           [ 0.5429,  0.3209, -0.6252]],\n",
      "\n",
      "          [[-1.1268,  1.1407,  0.5921],\n",
      "           [ 0.4200,  0.1327, -0.1222],\n",
      "           [ 1.3568,  0.2071, -0.4168],\n",
      "           [ 0.0951,  0.3322,  0.2183]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-1.0289,  0.5725,  0.2907],\n",
      "           [-0.8497, -0.5494,  1.0104],\n",
      "           [-0.3799, -1.5127, -0.4446],\n",
      "           [ 0.1473,  0.6119,  0.4588]],\n",
      "\n",
      "          [[ 0.3723,  0.7210,  0.5929],\n",
      "           [ 0.8182, -0.1323,  0.0854],\n",
      "           [-0.3233, -0.8016, -0.7996],\n",
      "           [-0.8222,  0.7653,  0.7958]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9926,  0.8681, -0.3637],\n",
      "           [-0.5099, -0.1323,  1.0253],\n",
      "           [-0.6232, -0.8120,  0.3218],\n",
      "           [-0.2394,  0.1995,  0.5550]],\n",
      "\n",
      "          [[-0.0956,  0.2703,  1.4685],\n",
      "           [ 0.4142,  0.4203,  0.7151],\n",
      "           [ 0.9828, -0.4855,  0.0317],\n",
      "           [ 0.8840, -0.0611,  0.6710]]]]], grad_fn=<PermuteBackward0>)\n",
      "After reshaping QKV: torch.Size([3, 2, 2, 4, 3])\n",
      "Q : \n",
      "tensor([[[[ 0.7425, -0.0707, -0.0828],\n",
      "          [ 0.3572, -0.8977,  0.0735],\n",
      "          [-0.4196, -0.7100, -0.6434],\n",
      "          [ 1.4142, -0.4784,  1.2242]],\n",
      "\n",
      "         [[ 0.4573,  0.4544,  1.0330],\n",
      "          [-0.6721,  0.5857,  0.6755],\n",
      "          [-1.4602,  0.5826,  0.2719],\n",
      "          [ 1.7068,  1.0256,  0.1558]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5831,  0.2470,  1.2699],\n",
      "          [ 0.9850, -0.6415,  0.7407],\n",
      "          [ 0.0825, -0.5945, -0.6763],\n",
      "          [ 0.2901, -0.4221,  0.3713]],\n",
      "\n",
      "         [[ 0.5537, -0.2212, -0.1294],\n",
      "          [ 0.1534,  0.8821,  0.7912],\n",
      "          [-1.0154, -0.1117,  0.6730],\n",
      "          [-0.3297, -0.0722,  0.3486]]]], grad_fn=<SelectBackward0>)\n",
      "K : \n",
      "tensor([[[[ 0.2471,  0.0914,  0.7612],\n",
      "          [ 1.1203, -0.0960, -0.2443],\n",
      "          [ 0.6467, -0.7578, -0.8227],\n",
      "          [-0.7472,  0.5901, -0.2340]],\n",
      "\n",
      "         [[-0.0415,  1.4642,  0.0638],\n",
      "          [ 0.9797, -0.5174, -0.4123],\n",
      "          [ 1.5211, -1.2511, -1.5030],\n",
      "          [-0.6541,  0.6875,  0.2207]]],\n",
      "\n",
      "\n",
      "        [[[-0.7502,  0.9520, -1.5822],\n",
      "          [ 0.7052,  0.2720, -0.3689],\n",
      "          [ 1.5108, -0.7995, -0.1226],\n",
      "          [ 0.5429,  0.3209, -0.6252]],\n",
      "\n",
      "         [[-1.1268,  1.1407,  0.5921],\n",
      "          [ 0.4200,  0.1327, -0.1222],\n",
      "          [ 1.3568,  0.2071, -0.4168],\n",
      "          [ 0.0951,  0.3322,  0.2183]]]], grad_fn=<SelectBackward0>)\n",
      "V : \n",
      "tensor([[[[-1.0289,  0.5725,  0.2907],\n",
      "          [-0.8497, -0.5494,  1.0104],\n",
      "          [-0.3799, -1.5127, -0.4446],\n",
      "          [ 0.1473,  0.6119,  0.4588]],\n",
      "\n",
      "         [[ 0.3723,  0.7210,  0.5929],\n",
      "          [ 0.8182, -0.1323,  0.0854],\n",
      "          [-0.3233, -0.8016, -0.7996],\n",
      "          [-0.8222,  0.7653,  0.7958]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9926,  0.8681, -0.3637],\n",
      "          [-0.5099, -0.1323,  1.0253],\n",
      "          [-0.6232, -0.8120,  0.3218],\n",
      "          [-0.2394,  0.1995,  0.5550]],\n",
      "\n",
      "         [[-0.0956,  0.2703,  1.4685],\n",
      "          [ 0.4142,  0.4203,  0.7151],\n",
      "          [ 0.9828, -0.4855,  0.0317],\n",
      "          [ 0.8840, -0.0611,  0.6710]]]], grad_fn=<SelectBackward0>)\n",
      "Query shape: torch.Size([2, 2, 4, 3])\n",
      "Key shape: torch.Size([2, 2, 4, 3])\n",
      "Value shape: torch.Size([2, 2, 4, 3])\n",
      "Attention scores shape: torch.Size([2, 2, 4, 4])\n",
      "Attention weights : \n",
      "tensor([[[[0.2206, 0.3391, 0.2923, 0.1480],\n",
      "          [0.2250, 0.2845, 0.3548, 0.1357],\n",
      "          [0.1643, 0.2086, 0.3805, 0.2466],\n",
      "          [0.3547, 0.3744, 0.2029, 0.0679]],\n",
      "\n",
      "         [[0.3789, 0.2221, 0.1103, 0.2887],\n",
      "          [0.4097, 0.1171, 0.0484, 0.4248],\n",
      "          [0.3834, 0.0772, 0.0322, 0.5072],\n",
      "          [0.3365, 0.2727, 0.2730, 0.1178]]],\n",
      "\n",
      "\n",
      "        [[[0.0812, 0.2928, 0.3948, 0.2313],\n",
      "          [0.0438, 0.2166, 0.5658, 0.1738],\n",
      "          [0.2564, 0.2160, 0.2946, 0.2330],\n",
      "          [0.1283, 0.2507, 0.3927, 0.2282]],\n",
      "\n",
      "         [[0.1362, 0.2680, 0.3662, 0.2296],\n",
      "          [0.3837, 0.1900, 0.1874, 0.2388],\n",
      "          [0.5157, 0.1684, 0.0863, 0.2296],\n",
      "          [0.3376, 0.2272, 0.1786, 0.2567]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Attention weights shape: torch.Size([2, 2, 4, 4])\n",
      "Weighted V : \n",
      "tensor([[[[-0.6043, -0.4117,  0.3447],\n",
      "          [-0.5881, -0.4811,  0.2574],\n",
      "          [-0.4545, -0.4452,  0.2025],\n",
      "          [-0.7502, -0.2680,  0.4224]],\n",
      "\n",
      "         [[ 0.0498,  0.3764,  0.3852],\n",
      "          [-0.1166,  0.5661,  0.5522],\n",
      "          [-0.2215,  0.6285,  0.6118],\n",
      "          [ 0.1633,  0.0778,  0.0983]]],\n",
      "\n",
      "\n",
      "        [[[-0.3701, -0.2426,  0.5260],\n",
      "          [-0.4612, -0.4154,  0.4847],\n",
      "          [-0.0950,  0.0013,  0.3524],\n",
      "          [-0.2998, -0.1951,  0.4634]],\n",
      "\n",
      "         [[ 0.6608, -0.0423,  0.5574],\n",
      "          [ 0.4373,  0.0780,  0.8656],\n",
      "          [ 0.3082,  0.1542,  1.0346],\n",
      "          [ 0.4642,  0.0843,  0.8361]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "After applying attention: torch.Size([2, 2, 4, 3])\n",
      "After combining heads: torch.Size([2, 4, 6])\n",
      "Output : \n",
      "tensor([[[-0.1287,  0.2238,  0.0826,  0.8456,  0.0175,  0.0828],\n",
      "         [-0.1723,  0.1345,  0.1571,  0.9808, -0.0023,  0.0745],\n",
      "         [-0.1874,  0.1316,  0.1997,  0.9828,  0.0348,  0.1363],\n",
      "         [-0.1472,  0.2522, -0.0576,  0.7093,  0.0516,  0.0595]],\n",
      "\n",
      "        [[ 0.2231,  0.5456, -0.0158,  0.4270, -0.0203,  0.2256],\n",
      "         [ 0.2139,  0.3932,  0.0410,  0.6369, -0.0652,  0.1743],\n",
      "         [ 0.1940,  0.4502,  0.0486,  0.5040,  0.0339,  0.4222],\n",
      "         [ 0.2088,  0.4690,  0.0202,  0.5271, -0.0141,  0.2862]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "After final projection: torch.Size([2, 4, 6])\n",
      "Final output shape: torch.Size([2, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Initial input tensor\n",
    "x = torch.randn(2, 4, 6)  # [batch_size, sequence_length, embedding_dim]\n",
    "print(\"Input : \")\n",
    "print(x)\n",
    "print(f\"Input shape: {x.shape}\")  # [2, 4, 6]\n",
    "\n",
    "# For our Attention module with 2 heads\n",
    "attention = Attention(dim=6, num_heads=2)\n",
    "\n",
    "# Step 1: QKV projection\n",
    "# self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "# Projects from [2, 4, 6] to [2, 4, 18] (6*3=18)\n",
    "qkv_projected = attention.qkv(x)\n",
    "print(\"QKV Projected : \")\n",
    "print(qkv_projected)\n",
    "print(f\"After QKV projection: {qkv_projected.shape}\")  # [2, 4, 18]\n",
    "\n",
    "# Step 2: Reshape QKV to separate heads\n",
    "# Rearrange from [2, 4, 18] to [3, 2, 2, 4, 3]\n",
    "# [3, batch, heads, sequence, head_dim]\n",
    "qkv = rearrange(qkv_projected, 'b n (three h d) -> three b h n d', three=3, h=2)\n",
    "print(\"QKV : \")\n",
    "print(qkv)\n",
    "print(f\"After reshaping QKV: {qkv.shape}\")  # [3, 2, 2, 4, 3]\n",
    "\n",
    "# Step 3: Separate Q, K, V\n",
    "q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "print(\"Q : \")\n",
    "print(q)\n",
    "print(\"K : \")\n",
    "print(k)\n",
    "print(\"V : \")\n",
    "print(v)\n",
    "print(f\"Query shape: {q.shape}\")  # [2, 2, 4, 3]\n",
    "print(f\"Key shape: {k.shape}\")    # [2, 2, 4, 3]\n",
    "print(f\"Value shape: {v.shape}\")  # [2, 2, 4, 3]\n",
    "\n",
    "# Step 4: Calculate attention scores\n",
    "# q @ k.transpose(-2, -1) means matrix multiplication of q and k transposed\n",
    "# q: [2, 2, 4, 3] and k.transpose: [2, 2, 3, 4]\n",
    "# Result: [2, 2, 4, 4] - each token attends to all other tokens\n",
    "scale = 3 ** -0.5  # Scale factor: 1/sqrt(head_dim)\n",
    "attn = (q @ k.transpose(-2, -1)) * scale\n",
    "print(f\"Attention scores shape: {attn.shape}\")  # [2, 2, 4, 4]\n",
    "\n",
    "# Step 5: Apply softmax to get attention weights\n",
    "attn = attn.softmax(dim=-1)\n",
    "print(\"Attention weights : \")\n",
    "print(attn)\n",
    "print(f\"Attention weights shape: {attn.shape}\")  # [2, 2, 4, 4]\n",
    "# The weights now sum to 1 along the last dimension\n",
    "\n",
    "# Step 6: Apply dropout (skipping actual dropout for clarity)\n",
    "# attn = attention.attn_drop(attn)\n",
    "\n",
    "# Step 7: Apply attention weights to values\n",
    "# attn: [2, 2, 4, 4] and v: [2, 2, 4, 3]\n",
    "# Result: [2, 2, 4, 3]\n",
    "weighted_v = attn @ v\n",
    "print(\"Weighted V : \")\n",
    "print(weighted_v)\n",
    "print(f\"After applying attention: {weighted_v.shape}\")  # [2, 2, 4, 3]\n",
    "\n",
    "# Step 8: Combine heads\n",
    "# Reshape from [2, 2, 4, 3] to [2, 4, 6]\n",
    "combined = rearrange(weighted_v, 'b h n d -> b n (h d)')\n",
    "print(f\"After combining heads: {combined.shape}\")  # [2, 4, 6]\n",
    "\n",
    "# Step 9: Final projection\n",
    "# self.proj = nn.Linear(dim, dim)\n",
    "# Projects from [2, 4, 6] to [2, 4, 6]\n",
    "output = attention.proj(combined)\n",
    "print(\"Output : \")\n",
    "print(output)\n",
    "print(f\"After final projection: {output.shape}\")  # [2, 4, 6]\n",
    "\n",
    "# Step 10: Apply dropout (skipping actual dropout for clarity)\n",
    "# output = attention.proj_drop(output)\n",
    "\n",
    "# Final output shape matches input shape\n",
    "print(f\"Final output shape: {output.shape}\")  # [2, 4, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a441cd",
   "metadata": {},
   "source": [
    "## MLP Block\n",
    "- **Input:** Normalized tokens (B, N+1, embed_dim)\n",
    "- **Process:**\n",
    "  - FC1: (B, N+1, embed_dim) → (B, N+1, hidden_dim) where hidden_dim = mlp_ratio * embed_dim\n",
    "  - Activation (GELU)\n",
    "  - Dropout\n",
    "  - FC2: (B, N+1, hidden_dim) → (B, N+1, embed_dim)\n",
    "  - Dropout\n",
    "- **Output:** MLP output (B, N+1, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dee844ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "        in_features : int,\n",
    "        hidden_features : Optional[int],\n",
    "        out_features : Optional[int],\n",
    "        activation : nn.Module = nn.GELU,\n",
    "        drop : float = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.activation = activation()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.drop1 = nn.Dropout(drop)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop2 = nn.Dropout(drop)\n",
    "    \n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb475a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3741,  0.3086, -0.2607, -0.4080],\n",
      "        [-0.7085, -1.5666, -0.3569,  0.7105]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(2, 4)\n",
    "mlp = MLP(4, 8, 4)\n",
    "print(mlp(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afb1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob: float = 0.):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        # work with diff dim tensors, not just 2D ConvNets\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()  # binarize\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output\n",
    "\n",
    "\n",
    "# Transformer Block\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim : int,\n",
    "            num_heads : int = 8,\n",
    "            mlp_ratio : float = 4,\n",
    "            qkv_bias : bool = True,\n",
    "            drop : float = 0.,\n",
    "            attn_drop : float = 0.,\n",
    "            drop_path : float = 0.,\n",
    "            act_layer : nn.Module = nn.GELU,\n",
    "            norm_layer : nn.Module = nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop\n",
    "        )\n",
    "    \n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb8c10",
   "metadata": {},
   "source": [
    "### 4. Final Layer Normalization\n",
    "- **Input:** Output from last encoder block (B, N+1, embed_dim)\n",
    "- **Output:** Normalized representation (B, N+1, embed_dim)\n",
    "\n",
    "### 5. Feature Extraction\n",
    "- For classification: Use only class token (B, 1, embed_dim) → (B, embed_dim)\n",
    "- For dense prediction: Use patch tokens (B, N, embed_dim)\n",
    "\n",
    "## Architecture Organization\n",
    "- **VisionTransformer (main class):**\n",
    "  - Contains PatchEmbed, ClassToken, PositionalEmbedding, Blocks, and final normalization\n",
    "  - Manages the overall forward pass through all components\n",
    "  \n",
    "- **PatchEmbed (component):**\n",
    "  - Handles the initial tokenization of the image\n",
    "\n",
    "- **Block (component):**\n",
    "  - Contains one complete transformer unit (Attention + MLP)\n",
    "  \n",
    "- **Attention (component):**\n",
    "  - Implements the multi-head self-attention mechanism\n",
    "  \n",
    "- **MLP (component):**\n",
    "  - Implements the feed-forward network in each block\n",
    "\n",
    "This sequence ensures the image is properly tokenized, enhanced with positional information, and processed through the self-attention mechanism to capture global relationships between patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783daa00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
